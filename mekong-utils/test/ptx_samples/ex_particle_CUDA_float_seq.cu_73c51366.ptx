//
// Generated by LLVM NVPTX Back-End
//

.version 3.2
.target sm_20
.address_size 64

	// .globl	_Z17calcLikelihoodSumPhPiii // -- Begin function _Z17calcLikelihoodSumPhPiii
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
// _ZZ24normalize_weights_kernelPdiS_S_S_PiE2u1 has been demoted
// _ZZ24normalize_weights_kernelPdiS_S_S_PiE10sumWeights has been demoted
// _ZZ17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S_E6buffer has been demoted
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
                                        // @_Z17calcLikelihoodSumPhPiii
.visible .func  (.param .b64 func_retval0) _Z17calcLikelihoodSumPhPiii(
	.param .b64 _Z17calcLikelihoodSumPhPiii_param_0,
	.param .b64 _Z17calcLikelihoodSumPhPiii_param_1,
	.param .b32 _Z17calcLikelihoodSumPhPiii_param_2,
	.param .b32 _Z17calcLikelihoodSumPhPiii_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<17>;

// %bb.0:                               // %entry
	ld.param.u32 	%r7, [_Z17calcLikelihoodSumPhPiii_param_2];
	setp.lt.s32 	%p1, %r7, 1;
	mov.f64 	%fd31, 0d0000000000000000;
	@%p1 bra 	LBB0_6;
// %bb.1:                               // %for.body.lr.ph
	ld.param.u32 	%r8, [_Z17calcLikelihoodSumPhPiii_param_3];
	ld.param.u64 	%rd5, [_Z17calcLikelihoodSumPhPiii_param_1];
	ld.param.u64 	%rd4, [_Z17calcLikelihoodSumPhPiii_param_0];
	mul.lo.s32 	%r1, %r8, %r7;
	and.b32  	%r2, %r7, 1;
	setp.eq.s32 	%p2, %r7, 1;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.u32 	%r22, 0;
	@%p2 bra 	LBB0_4;
// %bb.2:                               // %for.body.lr.ph.new
	sub.s32 	%r3, %r7, %r2;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	add.s64 	%rd16, %rd7, 4;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.u32 	%r22, 0;
LBB0_3:                                 // %for.body
                                        // =>This Inner Loop Header: Depth=1
	ld.s32 	%rd8, [%rd16+-4];
	add.s64 	%rd9, %rd4, %rd8;
	ld.u8 	%r11, [%rd9];
	add.s32 	%r12, %r11, -100;
	cvt.rn.f64.s32 	%fd11, %r12;
	add.s32 	%r13, %r11, -228;
	cvt.rn.f64.s32 	%fd12, %r13;
	mul.rn.f64 	%fd13, %fd12, %fd12;
	neg.f64 	%fd14, %fd13;
	fma.rn.f64 	%fd15, %fd11, %fd11, %fd14;
	div.rn.f64 	%fd16, %fd15, 0d4049000000000000;
	add.rn.f64 	%fd17, %fd31, %fd16;
	ld.s32 	%rd10, [%rd16];
	add.s64 	%rd11, %rd4, %rd10;
	ld.u8 	%r14, [%rd11];
	add.s32 	%r15, %r14, -100;
	cvt.rn.f64.s32 	%fd18, %r15;
	add.s32 	%r16, %r14, -228;
	cvt.rn.f64.s32 	%fd19, %r16;
	mul.rn.f64 	%fd20, %fd19, %fd19;
	neg.f64 	%fd21, %fd20;
	fma.rn.f64 	%fd22, %fd18, %fd18, %fd21;
	div.rn.f64 	%fd23, %fd22, 0d4049000000000000;
	add.rn.f64 	%fd31, %fd17, %fd23;
	add.s32 	%r22, %r22, 2;
	add.s64 	%rd16, %rd16, 8;
	setp.ne.s32 	%p3, %r3, %r22;
	@%p3 bra 	LBB0_3;
LBB0_4:                                 // %for.end.loopexit.unr-lcssa
	setp.eq.s32 	%p4, %r2, 0;
	@%p4 bra 	LBB0_6;
// %bb.5:                               // %for.body.epil
	add.s32 	%r17, %r22, %r1;
	mul.wide.s32 	%rd12, %r17, 4;
	add.s64 	%rd13, %rd5, %rd12;
	ld.s32 	%rd14, [%rd13];
	add.s64 	%rd15, %rd4, %rd14;
	ld.u8 	%r18, [%rd15];
	add.s32 	%r19, %r18, -100;
	cvt.rn.f64.s32 	%fd24, %r19;
	add.s32 	%r20, %r18, -228;
	cvt.rn.f64.s32 	%fd25, %r20;
	mul.rn.f64 	%fd26, %fd25, %fd25;
	neg.f64 	%fd27, %fd26;
	fma.rn.f64 	%fd28, %fd24, %fd24, %fd27;
	div.rn.f64 	%fd29, %fd28, 0d4049000000000000;
	add.rn.f64 	%fd31, %fd31, %fd29;
LBB0_6:                                 // %for.end
	st.param.f64 	[func_retval0+0], %fd31;
	ret;
                                        // -- End function
}
	// .globl	_Z7cdfCalcPdS_i // -- Begin function _Z7cdfCalcPdS_i
.visible .func _Z7cdfCalcPdS_i(
	.param .b64 _Z7cdfCalcPdS_i_param_0,
	.param .b64 _Z7cdfCalcPdS_i_param_1,
	.param .b32 _Z7cdfCalcPdS_i_param_2
)                                       // @_Z7cdfCalcPdS_i
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<28>;
	.reg .b64 	%rd<21>;

// %bb.0:                               // %entry
	ld.param.u32 	%r9, [_Z7cdfCalcPdS_i_param_2];
	ld.param.u64 	%rd15, [_Z7cdfCalcPdS_i_param_1];
	ld.param.u64 	%rd14, [_Z7cdfCalcPdS_i_param_0];
	ld.u64 	%rd1, [%rd15];
	st.u64 	[%rd14], %rd1;
	setp.lt.s32 	%p1, %r9, 2;
	@%p1 bra 	LBB1_8;
// %bb.1:                               // %for.body.preheader
	mov.b64 	%fd7, %rd1;
	ld.f64 	%fd8, [%rd15+8];
	add.rn.f64 	%fd26, %fd8, %fd7;
	st.f64 	[%rd14+8], %fd26;
	setp.eq.s32 	%p2, %r9, 2;
	@%p2 bra 	LBB1_8;
// %bb.2:                               // %for.body.for.body_crit_edge.lr.ph
	add.s32 	%r11, %r9, -2;
	add.s32 	%r12, %r9, -3;
	and.b32  	%r1, %r11, 7;
	setp.lt.u32 	%p3, %r12, 7;
	mov.u32 	%r15, 2;
	@%p3 bra 	LBB1_5;
// %bb.3:                               // %for.body.for.body_crit_edge.lr.ph.new
	sub.s32 	%r2, %r9, %r1;
	add.s64 	%rd18, %rd15, 40;
	add.s64 	%rd17, %rd14, 40;
	mov.u32 	%r15, 2;
LBB1_4:                                 // %for.body.for.body_crit_edge
                                        // =>This Inner Loop Header: Depth=1
	ld.f64 	%fd9, [%rd18+-24];
	add.rn.f64 	%fd10, %fd9, %fd26;
	st.f64 	[%rd17+-24], %fd10;
	ld.f64 	%fd11, [%rd18+-16];
	add.rn.f64 	%fd12, %fd11, %fd10;
	st.f64 	[%rd17+-16], %fd12;
	ld.f64 	%fd13, [%rd18+-8];
	add.rn.f64 	%fd14, %fd13, %fd12;
	st.f64 	[%rd17+-8], %fd14;
	ld.f64 	%fd15, [%rd18];
	add.rn.f64 	%fd16, %fd15, %fd14;
	st.f64 	[%rd17], %fd16;
	ld.f64 	%fd17, [%rd18+8];
	add.rn.f64 	%fd18, %fd17, %fd16;
	st.f64 	[%rd17+8], %fd18;
	ld.f64 	%fd19, [%rd18+16];
	add.rn.f64 	%fd20, %fd19, %fd18;
	st.f64 	[%rd17+16], %fd20;
	ld.f64 	%fd21, [%rd18+24];
	add.rn.f64 	%fd22, %fd21, %fd20;
	st.f64 	[%rd17+24], %fd22;
	ld.f64 	%fd23, [%rd18+32];
	add.rn.f64 	%fd26, %fd23, %fd22;
	st.f64 	[%rd17+32], %fd26;
	add.s32 	%r15, %r15, 8;
	add.s64 	%rd18, %rd18, 64;
	add.s64 	%rd17, %rd17, 64;
	setp.ne.s32 	%p4, %r2, %r15;
	@%p4 bra 	LBB1_4;
LBB1_5:                                 // %for.end.loopexit.unr-lcssa
	setp.eq.s32 	%p5, %r1, 0;
	@%p5 bra 	LBB1_8;
// %bb.6:                               // %for.body.for.body_crit_edge.epil.preheader
	mul.wide.u32 	%rd16, %r15, 8;
	add.s64 	%rd20, %rd14, %rd16;
	add.s64 	%rd19, %rd15, %rd16;
	neg.s32 	%r16, %r1;
LBB1_7:                                 // %for.body.for.body_crit_edge.epil
                                        // =>This Inner Loop Header: Depth=1
	.pragma "nounroll";
	ld.f64 	%fd24, [%rd19];
	add.rn.f64 	%fd26, %fd24, %fd26;
	st.f64 	[%rd20], %fd26;
	add.s64 	%rd20, %rd20, 8;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r16, %r16, 1;
	setp.ne.s32 	%p6, %r16, 0;
	@%p6 bra 	LBB1_7;
LBB1_8:                                 // %for.end
	ret;
                                        // -- End function
}
	// .globl	_Z7d_randuPii   // -- Begin function _Z7d_randuPii
.visible .func  (.param .b64 func_retval0) _Z7d_randuPii(
	.param .b64 _Z7d_randuPii_param_0,
	.param .b32 _Z7d_randuPii_param_1
)                                       // @_Z7d_randuPii
{
	.reg .b32 	%r<9>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<7>;

// %bb.0:                               // %entry
	ld.param.u64 	%rd1, [_Z7d_randuPii_param_0];
	ld.param.u32 	%r1, [_Z7d_randuPii_param_1];
	mul.wide.s32 	%rd2, %r1, 4;
	add.s64 	%rd3, %rd1, %rd2;
	ld.u32 	%r2, [%rd3];
	mad.lo.s32 	%r3, %r2, 1103515245, 12345;
	mul.wide.s32 	%rd4, %r3, 1073741825;
	shr.u64 	%rd5, %rd4, 63;
	cvt.u32.u64 	%r4, %rd5;
	shr.s64 	%rd6, %rd4, 61;
	cvt.u32.u64 	%r5, %rd6;
	add.s32 	%r6, %r5, %r4;
	mul.lo.s32 	%r7, %r6, 2147483647;
	sub.s32 	%r8, %r3, %r7;
	st.u32 	[%rd3], %r8;
	cvt.rn.f64.s32 	%fd1, %r8;
	div.rn.f64 	%fd2, %fd1, 0d41DFFFFFFFC00000;
	abs.f64 	%fd3, %fd2;
	st.param.f64 	[func_retval0+0], %fd3;
	ret;
                                        // -- End function
}
	// .globl	_Z7d_randnPii   // -- Begin function _Z7d_randnPii
.visible .func  (.param .b64 func_retval0) _Z7d_randnPii(
	.param .b64 _Z7d_randnPii_param_0,
	.param .b32 _Z7d_randnPii_param_1
)                                       // @_Z7d_randnPii
{
	.local .align 4 .b8 	__local_depot3[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<55>;
	.reg .f64 	%fd<86>;
	.reg .b64 	%rd<16>;

// %bb.0:                               // %entry
	mov.u64 	%SPL, __local_depot3;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [_Z7d_randnPii_param_0];
	ld.param.u32 	%r14, [_Z7d_randnPii_param_1];
	add.u64 	%rd3, %SP, 0;
	add.u64 	%rd1, %SPL, 0;
	mul.wide.s32 	%rd4, %r14, 4;
	add.s64 	%rd5, %rd2, %rd4;
	ld.u32 	%r15, [%rd5];
	mad.lo.s32 	%r16, %r15, 1103515245, 12345;
	mul.wide.s32 	%rd6, %r16, 1073741825;
	shr.u64 	%rd7, %rd6, 63;
	cvt.u32.u64 	%r17, %rd7;
	shr.s64 	%rd8, %rd6, 61;
	cvt.u32.u64 	%r18, %rd8;
	add.s32 	%r19, %r18, %r17;
	mul.lo.s32 	%r20, %r19, 2147483647;
	sub.s32 	%r21, %r16, %r20;
	cvt.rn.f64.s32 	%fd15, %r21;
	div.rn.f64 	%fd16, %fd15, 0d41DFFFFFFFC00000;
	abs.f64 	%fd83, %fd16;
	mad.lo.s32 	%r22, %r21, 1103515245, 12345;
	mul.wide.s32 	%rd9, %r22, 1073741825;
	shr.u64 	%rd10, %rd9, 63;
	cvt.u32.u64 	%r23, %rd10;
	shr.s64 	%rd11, %rd9, 61;
	cvt.u32.u64 	%r24, %rd11;
	add.s32 	%r25, %r24, %r23;
	mul.lo.s32 	%r26, %r25, 2147483647;
	sub.s32 	%r27, %r22, %r26;
	st.u32 	[%rd5], %r27;
	cvt.rn.f64.s32 	%fd17, %r27;
	div.rn.f64 	%fd18, %fd17, 0d41DFFFFFFFC00000;
	abs.f64 	%fd19, %fd18;
	mul.rn.f64 	%fd20, %fd19, 0d401921FB54442D18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd20;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd20;
	}
	and.b32  	%r30, %r29, 2147483647;
	setp.eq.s32 	%p1, %r30, 2146435072;
	setp.eq.s32 	%p2, %r28, 0;
	mul.rn.f64 	%fd21, %fd20, 0d0000000000000000;
	selp.f64 	%fd22, %fd21, %fd20, %p1;
	selp.f64 	%fd2, %fd22, %fd20, %p2;
	mul.rn.f64 	%fd23, %fd2, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r50, %fd23;
	st.local.u32 	[%rd1], %r50;
	cvt.rn.f64.s32 	%fd24, %r50;
	fma.rn.f64 	%fd25, %fd24, 0dBFF921FB54442D18, %fd2;
	fma.rn.f64 	%fd26, %fd24, 0dBC91A62633145C00, %fd25;
	fma.rn.f64 	%fd82, %fd24, 0dB97B839A252049C0, %fd26;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd2;
	}
	and.b32  	%r32, %r31, 2145386496;
	setp.lt.u32 	%p3, %r32, 1105199104;
	@%p3 bra 	LBB3_2;
// %bb.1:
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd82, [retval0+0];
	} // callseq 0
	ld.local.u32 	%r50, [%rd1];
LBB3_2:                                 // %_ZL3cosd.exit
	add.s32 	%r34, %r50, 1;
	and.b32  	%r35, %r34, 1;
	shl.b32 	%r36, %r35, 3;
	mul.wide.u32 	%rd13, %r36, 8;
	mov.u64 	%rd14, __cudart_sin_cos_coeffs;
	add.s64 	%rd15, %rd14, %rd13;
	mul.rn.f64 	%fd27, %fd82, %fd82;
	setp.eq.b32 	%p4, %r35, 1;
	selp.f64 	%fd28, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p4;
	ld.const.f64 	%fd29, [%rd15+8];
	fma.rn.f64 	%fd30, %fd28, %fd27, %fd29;
	ld.const.f64 	%fd31, [%rd15+16];
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	ld.const.f64 	%fd33, [%rd15+24];
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	ld.const.f64 	%fd35, [%rd15+32];
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	ld.const.f64 	%fd37, [%rd15+40];
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	ld.const.f64 	%fd39, [%rd15+48];
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	fma.rn.f64 	%fd41, %fd40, %fd82, %fd82;
	fma.rn.f64 	%fd42, %fd40, %fd27, 0d3FF0000000000000;
	selp.f64 	%fd43, %fd42, %fd41, %p4;
	and.b32  	%r37, %r34, 2;
	setp.eq.s32 	%p5, %r37, 0;
	mov.f64 	%fd44, 0d0000000000000000;
	sub.rn.f64 	%fd45, %fd44, %fd43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd83;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r52, %temp}, %fd83;
	}
	setp.gt.s32 	%p6, %r51, 1048575;
	mov.u32 	%r53, -1023;
	@%p6 bra 	LBB3_4;
// %bb.3:
	mul.rn.f64 	%fd83, %fd83, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd83;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r52, %temp}, %fd83;
	}
	mov.u32 	%r53, -1077;
LBB3_4:
	selp.f64 	%fd6, %fd43, %fd45, %p5;
	add.s32 	%r39, %r51, -1;
	setp.gt.u32 	%p7, %r39, 2146435070;
	@%p7 bra 	LBB3_8;
// %bb.5:
	shr.u32 	%r41, %r51, 20;
	add.s32 	%r54, %r53, %r41;
	and.b32  	%r42, %r51, -2146435073;
	or.b32  	%r43, %r42, 1072693248;
	mov.b64 	%fd84, {%r52, %r43};
	setp.lt.s32 	%p9, %r43, 1073127583;
	@%p9 bra 	LBB3_7;
// %bb.6:
	add.s32 	%r54, %r54, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd84;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd84;
	}
	add.s32 	%r46, %r45, -1048576;
	mov.b64 	%fd84, {%r44, %r46};
LBB3_7:
	add.rn.f64 	%fd47, %fd84, 0dBFF0000000000000;
	add.rn.f64 	%fd48, %fd84, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd49, %fd48;
	neg.f64 	%fd50, %fd48;
	fma.rn.f64 	%fd51, %fd50, %fd49, 0d3FF0000000000000;
	fma.rn.f64 	%fd52, %fd51, %fd51, %fd51;
	fma.rn.f64 	%fd53, %fd52, %fd49, %fd49;
	mul.rn.f64 	%fd54, %fd47, %fd53;
	add.rn.f64 	%fd55, %fd54, %fd54;
	mul.rn.f64 	%fd56, %fd55, %fd55;
	fma.rn.f64 	%fd57, %fd56, 0d3EB1380B3AE80F1E, 0d3ED0EE258B7A8B04;
	fma.rn.f64 	%fd58, %fd57, %fd56, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd59, %fd58, %fd56, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd60, %fd59, %fd56, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd61, %fd60, %fd56, 0d3F624924923BE72D;
	fma.rn.f64 	%fd62, %fd61, %fd56, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd63, %fd62, %fd56, 0d3FB5555555555554;
	sub.rn.f64 	%fd64, %fd47, %fd55;
	add.rn.f64 	%fd65, %fd64, %fd64;
	neg.f64 	%fd66, %fd55;
	fma.rn.f64 	%fd67, %fd66, %fd47, %fd65;
	mul.rn.f64 	%fd68, %fd53, %fd67;
	mul.rn.f64 	%fd69, %fd56, %fd63;
	fma.rn.f64 	%fd70, %fd69, %fd55, %fd68;
	xor.b32  	%r47, %r54, -2147483648;
	mov.u32 	%r48, 1127219200;
	mov.b64 	%fd71, {%r47, %r48};
	mov.u32 	%r49, -2147483648;
	mov.b64 	%fd72, {%r49, %r48};
	sub.rn.f64 	%fd73, %fd71, %fd72;
	fma.rn.f64 	%fd74, %fd73, 0d3FE62E42FEFA39EF, %fd55;
	fma.rn.f64 	%fd75, %fd73, 0dBFE62E42FEFA39EF, %fd74;
	sub.rn.f64 	%fd76, %fd75, %fd55;
	sub.rn.f64 	%fd77, %fd70, %fd76;
	fma.rn.f64 	%fd78, %fd73, 0d3C7ABC9E3B39803F, %fd77;
	add.rn.f64 	%fd85, %fd74, %fd78;
	bra.uni 	LBB3_9;
LBB3_8:
	fma.rn.f64 	%fd46, %fd83, 0d7FF0000000000000, 0d7FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd83;
	}
	mov.b32 	%f1, %r40;
	setp.eq.f32 	%p8, %f1, 0f00000000;
	selp.f64 	%fd85, 0dFFF0000000000000, %fd46, %p8;
LBB3_9:                                 // %_ZL3logd.exit
	mul.rn.f64 	%fd79, %fd85, 0dC000000000000000;
	sqrt.rn.f64 	%fd80, %fd79;
	mul.rn.f64 	%fd81, %fd6, %fd80;
	st.param.f64 	[func_retval0+0], %fd81;
	ret;
                                        // -- End function
}
	// .globl	_Z13updateWeightsPdS_i // -- Begin function _Z13updateWeightsPdS_i
.visible .func  (.param .b64 func_retval0) _Z13updateWeightsPdS_i(
	.param .b64 _Z13updateWeightsPdS_i_param_0,
	.param .b64 _Z13updateWeightsPdS_i_param_1,
	.param .b32 _Z13updateWeightsPdS_i_param_2
)                                       // @_Z13updateWeightsPdS_i
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<9>;

// %bb.0:                               // %entry
	ld.param.u32 	%r20, [_Z13updateWeightsPdS_i_param_2];
	setp.lt.s32 	%p1, %r20, 1;
	mov.f64 	%fd33, 0d0000000000000000;
	@%p1 bra 	LBB4_6;
// %bb.1:                               // %for.body.preheader
	ld.param.u64 	%rd8, [_Z13updateWeightsPdS_i_param_1];
	ld.param.u64 	%rd7, [_Z13updateWeightsPdS_i_param_0];
	mov.f64 	%fd33, 0d0000000000000000;
LBB4_2:                                 // %for.body
                                        // =>This Inner Loop Header: Depth=1
	ld.f64 	%fd2, [%rd7];
	ld.f64 	%fd3, [%rd8];
	fma.rn.f64 	%fd12, %fd3, 0d3FF71547652B82FE, 0d4338000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd12;
	}
	add.rn.f64 	%fd13, %fd12, 0dC338000000000000;
	fma.rn.f64 	%fd14, %fd13, 0dBFE62E42FEFA39EF, %fd3;
	fma.rn.f64 	%fd15, %fd13, 0dBC7ABC9E3B39803F, %fd14;
	fma.rn.f64 	%fd16, %fd15, 0d3E5ADE1569CE2BDF, 0d3E928AF3FCA213EA;
	fma.rn.f64 	%fd17, %fd16, %fd15, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd18, %fd17, %fd15, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd19, %fd18, %fd15, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd20, %fd19, %fd15, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd21, %fd20, %fd15, 0d3F81111111122322;
	fma.rn.f64 	%fd22, %fd21, %fd15, 0d3FA55555555502A1;
	fma.rn.f64 	%fd23, %fd22, %fd15, 0d3FC5555555555511;
	fma.rn.f64 	%fd24, %fd23, %fd15, 0d3FE000000000000B;
	fma.rn.f64 	%fd25, %fd24, %fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd26, %fd25, %fd15, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd26;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd26;
	}
	shl.b32 	%r7, %r2, 20;
	add.s32 	%r8, %r4, %r7;
	mov.b64 	%fd32, {%r3, %r8};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd3;
	}
	and.b32  	%r10, %r9, 2147483647;
	mov.b32 	%f1, %r10;
	setp.lt.f32 	%p2, %f1, 0f4086232B;
	@%p2 bra 	LBB4_5;
// %bb.3:                               // %__internal_fast_icmp_abs_lt.exit.i.i
                                        //   in Loop: Header=BB4_2 Depth=1
	setp.lt.f64 	%p3, %fd3, 0d0000000000000000;
	add.rn.f64 	%fd27, %fd3, 0d7FF0000000000000;
	selp.f64 	%fd32, 0d0000000000000000, %fd27, %p3;
	setp.geu.f32 	%p4, %f1, 0f40874800;
	@%p4 bra 	LBB4_5;
// %bb.4:                               //   in Loop: Header=BB4_2 Depth=1
	shr.u32 	%r11, %r2, 31;
	add.s32 	%r12, %r2, %r11;
	shr.s32 	%r13, %r12, 1;
	shl.b32 	%r14, %r13, 20;
	add.s32 	%r15, %r4, %r14;
	mov.b64 	%fd28, {%r3, %r15};
	sub.s32 	%r16, %r2, %r13;
	shl.b32 	%r17, %r16, 20;
	add.s32 	%r18, %r17, 1072693248;
	mov.u32 	%r19, 0;
	mov.b64 	%fd29, {%r19, %r18};
	mul.rn.f64 	%fd32, %fd28, %fd29;
LBB4_5:                                 // %_ZL3expd.exit
                                        //   in Loop: Header=BB4_2 Depth=1
	mul.rn.f64 	%fd30, %fd2, %fd32;
	st.f64 	[%rd7], %fd30;
	fma.rn.f64 	%fd33, %fd2, %fd32, %fd33;
	add.s32 	%r20, %r20, -1;
	add.s64 	%rd8, %rd8, 8;
	add.s64 	%rd7, %rd7, 8;
	setp.ne.s32 	%p5, %r20, 0;
	@%p5 bra 	LBB4_2;
LBB4_6:                                 // %for.end
	st.param.f64 	[func_retval0+0], %fd33;
	ret;
                                        // -- End function
}
	// .globl	_Z12findIndexBinPdiid // -- Begin function _Z12findIndexBinPdiid
.visible .func  (.param .b32 func_retval0) _Z12findIndexBinPdiid(
	.param .b64 _Z12findIndexBinPdiid_param_0,
	.param .b32 _Z12findIndexBinPdiid_param_1,
	.param .b32 _Z12findIndexBinPdiid_param_2,
	.param .b64 _Z12findIndexBinPdiid_param_3
)                                       // @_Z12findIndexBinPdiid
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<11>;

// %bb.0:                               // %entry
	ld.param.u32 	%r24, [_Z12findIndexBinPdiid_param_2];
	ld.param.u32 	%r25, [_Z12findIndexBinPdiid_param_1];
	setp.le.s32 	%p1, %r24, %r25;
	mov.u32 	%r16, -1;
	mov.u32 	%r29, %r16;
	@%p1 bra 	LBB5_12;
// %bb.1:                               // %while.body.preheader
	ld.param.f64 	%fd3, [_Z12findIndexBinPdiid_param_3];
	ld.param.u64 	%rd6, [_Z12findIndexBinPdiid_param_0];
LBB5_2:                                 // %while.body
                                        // =>This Inner Loop Header: Depth=1
	sub.s32 	%r17, %r24, %r25;
	shr.u32 	%r18, %r17, 31;
	add.s32 	%r19, %r17, %r18;
	shr.s32 	%r20, %r19, 1;
	add.s32 	%r27, %r20, %r25;
	mul.wide.s32 	%rd7, %r27, 8;
	add.s64 	%rd2, %rd6, %rd7;
	ld.f64 	%fd1, [%rd2];
	setp.ge.f64 	%p2, %fd1, %fd3;
	@%p2 bra 	LBB5_5;
// %bb.3:                               // %while.body.if.end26_crit_edge
                                        //   in Loop: Header=BB5_2 Depth=1
	add.s32 	%r28, %r27, -1;
	bra.uni 	LBB5_4;
LBB5_5:                                 // %if.then3
                                        //   in Loop: Header=BB5_2 Depth=1
	setp.eq.s32 	%p3, %r27, 0;
	mov.u32 	%r29, 0;
	@%p3 bra 	LBB5_12;
// %bb.6:                               // %if.else
                                        //   in Loop: Header=BB5_2 Depth=1
	ld.f64 	%fd2, [%rd2+-8];
	setp.lt.f64 	%p4, %fd2, %fd3;
	mov.u32 	%r29, %r27;
	@%p4 bra 	LBB5_12;
// %bb.7:                               // %if.else11
                                        //   in Loop: Header=BB5_2 Depth=1
	add.s32 	%r28, %r27, -1;
	setp.neu.f64 	%p5, %fd2, %fd3;
	@%p5 bra 	LBB5_4;
	bra.uni 	LBB5_8;
LBB5_4:                                 // %if.end26
                                        //   in Loop: Header=BB5_2 Depth=1
	setp.gt.f64 	%p12, %fd1, %fd3;
	add.s32 	%r23, %r27, 1;
	selp.b32 	%r25, %r25, %r23, %p12;
	selp.b32 	%r24, %r28, %r24, %p12;
	setp.gt.s32 	%p13, %r24, %r25;
	mov.u32 	%r29, %r16;
	@%p13 bra 	LBB5_2;
	bra.uni 	LBB5_12;
LBB5_8:                                 // %while.cond17.preheader
	setp.neu.f64 	%p6, %fd1, %fd3;
	setp.lt.s32 	%p7, %r27, 0;
	or.pred  	%p8, %p7, %p6;
	@%p8 bra 	LBB5_11;
// %bb.9:                               // %while.cond17.while.cond17_crit_edge.preheader
	cvt.s64.s32 	%rd1, %r27;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd6, %rd8;
	add.s64 	%rd10, %rd9, -8;
	mov.u32 	%r26, %r27;
LBB5_10:                                // %while.cond17.while.cond17_crit_edge
                                        // =>This Inner Loop Header: Depth=1
	add.s32 	%r27, %r26, -1;
	ld.f64 	%fd4, [%rd10];
	setp.eq.f64 	%p9, %fd4, %fd3;
	setp.gt.s32 	%p10, %r26, 0;
	and.pred  	%p11, %p10, %p9;
	add.s64 	%rd10, %rd10, -8;
	mov.u32 	%r26, %r27;
	@%p11 bra 	LBB5_10;
LBB5_11:                                // %while.end
	add.s32 	%r29, %r27, 1;
LBB5_12:                                // %return
	st.param.b32 	[func_retval0+0], %r29;
	ret;
                                        // -- End function
}
	// .globl	_Z16dev_round_doubled // -- Begin function _Z16dev_round_doubled
.visible .func  (.param .b64 func_retval0) _Z16dev_round_doubled(
	.param .b64 _Z16dev_round_doubled_param_0
)                                       // @_Z16dev_round_doubled
{
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<3>;

// %bb.0:                               // %entry
	ld.param.f64 	%fd1, [_Z16dev_round_doubled_param_0];
	cvt.rzi.s32.f64 	%r1, %fd1;
	cvt.rn.f64.s32 	%fd2, %r1;
	st.param.f64 	[func_retval0+0], %fd2;
	ret;
                                        // -- End function
}
	// .globl	_Z17find_index_kernelPdS_S_S_S_S_S_i // -- Begin function _Z17find_index_kernelPdS_S_S_S_S_S_i
.visible .entry _Z17find_index_kernelPdS_S_S_S_S_S_i(
	.param .u64 _Z17find_index_kernelPdS_S_S_S_S_S_i_param_0,
	.param .u64 _Z17find_index_kernelPdS_S_S_S_S_S_i_param_1,
	.param .u64 _Z17find_index_kernelPdS_S_S_S_S_S_i_param_2,
	.param .u64 _Z17find_index_kernelPdS_S_S_S_S_S_i_param_3,
	.param .u64 _Z17find_index_kernelPdS_S_S_S_S_S_i_param_4,
	.param .u64 _Z17find_index_kernelPdS_S_S_S_S_S_i_param_5,
	.param .u64 _Z17find_index_kernelPdS_S_S_S_S_S_i_param_6,
	.param .u32 _Z17find_index_kernelPdS_S_S_S_S_S_i_param_7
)                                       // @_Z17find_index_kernelPdS_S_S_S_S_S_i
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<27>;

// %bb.0:                               // %entry
	ld.param.u32 	%r6, [_Z17find_index_kernelPdS_S_S_S_S_S_i_param_7];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r8, %r7, %r9;
	setp.ge.s32 	%p1, %r1, %r6;
	@%p1 bra 	LBB7_7;
// %bb.1:                               // %for.cond.preheader
	ld.param.u64 	%rd10, [_Z17find_index_kernelPdS_S_S_S_S_S_i_param_0];
	ld.param.u64 	%rd11, [_Z17find_index_kernelPdS_S_S_S_S_S_i_param_5];
	cvta.to.global.u64 	%rd1, %rd11;
	ld.param.u64 	%rd12, [_Z17find_index_kernelPdS_S_S_S_S_S_i_param_1];
	ld.param.u64 	%rd13, [_Z17find_index_kernelPdS_S_S_S_S_S_i_param_4];
	cvta.to.global.u64 	%rd2, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd10;
	setp.lt.s32 	%p2, %r6, 1;
	cvt.s64.s32 	%rd7, %r1;
	@%p2 bra 	LBB7_5;
// %bb.2:                               // %for.body.lr.ph
	ld.param.u64 	%rd14, [_Z17find_index_kernelPdS_S_S_S_S_S_i_param_2];
	ld.param.u64 	%rd15, [_Z17find_index_kernelPdS_S_S_S_S_S_i_param_3];
	cvta.to.global.u64 	%rd3, %rd15;
	cvta.to.global.u64 	%rd26, %rd14;
	shl.b64 	%rd16, %rd7, 3;
	add.s64 	%rd17, %rd3, %rd16;
	ld.global.f64 	%fd1, [%rd17];
	mov.u32 	%r12, 0;
LBB7_3:                                 // %for.body
                                        // =>This Inner Loop Header: Depth=1
	ld.global.f64 	%fd2, [%rd26];
	setp.ge.f64 	%p3, %fd2, %fd1;
	@%p3 bra 	LBB7_6;
// %bb.4:                               // %for.inc
                                        //   in Loop: Header=BB7_3 Depth=1
	add.s32 	%r12, %r12, 1;
	add.s64 	%rd26, %rd26, 8;
	setp.lt.s32 	%p4, %r12, %r6;
	@%p4 bra 	LBB7_3;
LBB7_5:                                 // %for.cond._crit_edge
	add.s32 	%r12, %r6, -1;
LBB7_6:                                 // %for.end
	mul.wide.s32 	%rd18, %r12, 8;
	add.s64 	%rd19, %rd6, %rd18;
	ld.global.u64 	%rd20, [%rd19];
	shl.b64 	%rd21, %rd7, 3;
	add.s64 	%rd22, %rd2, %rd21;
	st.global.u64 	[%rd22], %rd20;
	add.s64 	%rd23, %rd5, %rd18;
	ld.global.u64 	%rd24, [%rd23];
	add.s64 	%rd25, %rd1, %rd21;
	st.global.u64 	[%rd25], %rd24;
LBB7_7:                                 // %if.end19
	bar.sync 	0;
	ret;
                                        // -- End function
}
	// .globl	_Z24normalize_weights_kernelPdiS_S_S_Pi // -- Begin function _Z24normalize_weights_kernelPdiS_S_S_Pi
.visible .entry _Z24normalize_weights_kernelPdiS_S_S_Pi(
	.param .u64 _Z24normalize_weights_kernelPdiS_S_S_Pi_param_0,
	.param .u32 _Z24normalize_weights_kernelPdiS_S_S_Pi_param_1,
	.param .u64 _Z24normalize_weights_kernelPdiS_S_S_Pi_param_2,
	.param .u64 _Z24normalize_weights_kernelPdiS_S_S_Pi_param_3,
	.param .u64 _Z24normalize_weights_kernelPdiS_S_S_Pi_param_4,
	.param .u64 _Z24normalize_weights_kernelPdiS_S_S_Pi_param_5
)                                       // @_Z24normalize_weights_kernelPdiS_S_S_Pi
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<40>;
	.reg .b64 	%rd<39>;
	// demoted variable
	.shared .align 8 .f64 _ZZ24normalize_weights_kernelPdiS_S_S_PiE2u1;
	// demoted variable
	.shared .align 8 .f64 _ZZ24normalize_weights_kernelPdiS_S_S_PiE10sumWeights;
// %bb.0:                               // %entry
	ld.param.u32 	%r11, [_Z24normalize_weights_kernelPdiS_S_S_Pi_param_1];
	ld.param.u64 	%rd22, [_Z24normalize_weights_kernelPdiS_S_S_Pi_param_0];
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r13, %r12, %r1;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	LBB8_2;
// %bb.1:                               // %if.then
	ld.param.u64 	%rd25, [_Z24normalize_weights_kernelPdiS_S_S_Pi_param_2];
	cvta.to.global.u64 	%rd4, %rd25;
	ld.global.u64 	%rd27, [%rd4];
	st.shared.u64 	[_ZZ24normalize_weights_kernelPdiS_S_S_PiE10sumWeights], %rd27;
LBB8_2:                                 // %if.end
	ld.param.u64 	%rd24, [_Z24normalize_weights_kernelPdiS_S_S_Pi_param_4];
	cvta.to.global.u64 	%rd5, %rd22;
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r11;
	@%p2 bra 	LBB8_4;
// %bb.3:                               // %if.then5
	mul.wide.s32 	%rd28, %r2, 8;
	add.s64 	%rd7, %rd5, %rd28;
	ld.global.f64 	%fd8, [%rd7];
	ld.shared.f64 	%fd9, [_ZZ24normalize_weights_kernelPdiS_S_S_PiE10sumWeights];
	div.rn.f64 	%fd10, %fd8, %fd9;
	st.global.f64 	[%rd7], %fd10;
LBB8_4:                                 // %if.end9
	cvta.to.global.u64 	%rd2, %rd24;
	bar.sync 	0;
	setp.ne.s32 	%p3, %r2, 0;
	@%p3 bra 	LBB8_13;
// %bb.5:                               // %if.then11
	ld.param.u64 	%rd23, [_Z24normalize_weights_kernelPdiS_S_S_Pi_param_5];
	ld.param.u64 	%rd26, [_Z24normalize_weights_kernelPdiS_S_S_Pi_param_3];
	cvta.to.global.u64 	%rd1, %rd23;
	cvta.to.global.u64 	%rd3, %rd26;
	ld.global.u64 	%rd8, [%rd5];
	st.global.u64 	[%rd3], %rd8;
	setp.lt.s32 	%p4, %r11, 2;
	@%p4 bra 	LBB8_12;
// %bb.6:                               // %for.body.preheader.i
	mov.b64 	%fd38, %rd8;
	add.s32 	%r15, %r11, -1;
	add.s32 	%r16, %r11, -2;
	and.b32  	%r3, %r15, 7;
	setp.lt.u32 	%p5, %r16, 7;
	mov.u32 	%r26, 1;
	@%p5 bra 	LBB8_9;
// %bb.7:                               // %for.body.preheader.i.new
	sub.s32 	%r4, %r11, %r3;
	add.s64 	%rd36, %rd3, 32;
	add.s64 	%rd35, %rd5, 32;
	mov.u32 	%r26, 1;
LBB8_8:                                 // %for.body.i
                                        // =>This Inner Loop Header: Depth=1
	ld.global.f64 	%fd11, [%rd35+-24];
	add.rn.f64 	%fd12, %fd38, %fd11;
	st.global.f64 	[%rd36+-24], %fd12;
	ld.global.f64 	%fd13, [%rd35+-16];
	add.rn.f64 	%fd14, %fd12, %fd13;
	st.global.f64 	[%rd36+-16], %fd14;
	ld.global.f64 	%fd15, [%rd35+-8];
	add.rn.f64 	%fd16, %fd14, %fd15;
	st.global.f64 	[%rd36+-8], %fd16;
	ld.global.f64 	%fd17, [%rd35];
	add.rn.f64 	%fd18, %fd16, %fd17;
	st.global.f64 	[%rd36], %fd18;
	ld.global.f64 	%fd19, [%rd35+8];
	add.rn.f64 	%fd20, %fd18, %fd19;
	st.global.f64 	[%rd36+8], %fd20;
	ld.global.f64 	%fd21, [%rd35+16];
	add.rn.f64 	%fd22, %fd20, %fd21;
	st.global.f64 	[%rd36+16], %fd22;
	ld.global.f64 	%fd23, [%rd35+24];
	add.rn.f64 	%fd24, %fd22, %fd23;
	st.global.f64 	[%rd36+24], %fd24;
	ld.global.f64 	%fd25, [%rd35+32];
	add.rn.f64 	%fd38, %fd24, %fd25;
	st.global.f64 	[%rd36+32], %fd38;
	add.s32 	%r26, %r26, 8;
	add.s64 	%rd36, %rd36, 64;
	add.s64 	%rd35, %rd35, 64;
	setp.ne.s32 	%p6, %r4, %r26;
	@%p6 bra 	LBB8_8;
LBB8_9:                                 // %_Z7cdfCalcPdS_i.exit.loopexit.unr-lcssa
	setp.eq.s32 	%p7, %r3, 0;
	@%p7 bra 	LBB8_12;
// %bb.10:                              // %for.body.i.epil.preheader
	mul.wide.u32 	%rd29, %r26, 8;
	add.s64 	%rd38, %rd5, %rd29;
	add.s64 	%rd37, %rd3, %rd29;
	neg.s32 	%r27, %r3;
LBB8_11:                                // %for.body.i.epil
                                        // =>This Inner Loop Header: Depth=1
	.pragma "nounroll";
	ld.global.f64 	%fd26, [%rd38];
	add.rn.f64 	%fd38, %fd38, %fd26;
	st.global.f64 	[%rd37], %fd38;
	add.s64 	%rd38, %rd38, 8;
	add.s64 	%rd37, %rd37, 8;
	add.s32 	%r27, %r27, 1;
	setp.ne.s32 	%p8, %r27, 0;
	@%p8 bra 	LBB8_11;
LBB8_12:                                // %_Z7cdfCalcPdS_i.exit
	cvt.rn.f64.s32 	%fd27, %r11;
	rcp.rn.f64 	%fd28, %fd27;
	ld.global.u32 	%r18, [%rd1];
	mad.lo.s32 	%r19, %r18, 1103515245, 12345;
	mul.wide.s32 	%rd30, %r19, 1073741825;
	shr.u64 	%rd31, %rd30, 63;
	cvt.u32.u64 	%r20, %rd31;
	shr.s64 	%rd32, %rd30, 61;
	cvt.u32.u64 	%r21, %rd32;
	add.s32 	%r22, %r21, %r20;
	mul.lo.s32 	%r23, %r22, 2147483647;
	sub.s32 	%r24, %r19, %r23;
	st.global.u32 	[%rd1], %r24;
	cvt.rn.f64.s32 	%fd29, %r24;
	div.rn.f64 	%fd30, %fd29, 0d41DFFFFFFFC00000;
	abs.f64 	%fd31, %fd30;
	mul.rn.f64 	%fd32, %fd28, %fd31;
	st.global.f64 	[%rd2], %fd32;
LBB8_13:                                // %if.end16
	bar.sync 	0;
	@%p1 bra 	LBB8_15;
// %bb.14:                              // %if.then19
	ld.global.u64 	%rd33, [%rd2];
	st.shared.u64 	[_ZZ24normalize_weights_kernelPdiS_S_S_PiE2u1], %rd33;
LBB8_15:                                // %if.end21
	bar.sync 	0;
	@%p2 bra 	LBB8_17;
// %bb.16:                              // %if.then23
	cvt.s64.s32 	%rd6, %r2;
	cvt.rn.f64.s32 	%fd33, %r2;
	cvt.rn.f64.s32 	%fd34, %r11;
	div.rn.f64 	%fd7, %fd33, %fd34;
	shl.b64 	%rd34, %rd6, 3;
	add.s64 	%rd21, %rd2, %rd34;
	ld.shared.f64 	%fd35, [_ZZ24normalize_weights_kernelPdiS_S_S_PiE2u1];
	add.rn.f64 	%fd36, %fd7, %fd35;
	st.global.f64 	[%rd21], %fd36;
LBB8_17:                                // %if.end30
	ret;
                                        // -- End function
}
	// .globl	_Z10sum_kernelPdi // -- Begin function _Z10sum_kernelPdi
.visible .entry _Z10sum_kernelPdi(
	.param .u64 _Z10sum_kernelPdi_param_0,
	.param .u32 _Z10sum_kernelPdi_param_1
)                                       // @_Z10sum_kernelPdi
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<36>;
	.reg .b64 	%rd<12>;

// %bb.0:                               // %entry
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mul.lo.s32 	%r13, %r12, %r11;
	mov.u32 	%r14, %tid.x;
	neg.s32 	%r15, %r14;
	setp.ne.s32 	%p1, %r13, %r15;
	@%p1 bra 	LBB9_9;
// %bb.1:                               // %if.then
	ld.param.u32 	%r10, [_Z10sum_kernelPdi_param_1];
	ld.param.u64 	%rd8, [_Z10sum_kernelPdi_param_0];
	cvta.to.global.u64 	%rd1, %rd8;
	cvt.rn.f64.s32 	%fd9, %r10;
	mul.rn.f64 	%fd10, %fd9, 0d3F60000000000000;
	cvt.rpi.f64.f64 	%fd11, %fd10;
	cvt.rzi.s32.f64 	%r1, %fd11;
	setp.lt.s32 	%p2, %r1, 1;
	mov.f64 	%fd35, 0d0000000000000000;
	@%p2 bra 	LBB9_8;
// %bb.2:                               // %for.body.preheader
	add.s32 	%r17, %r1, -1;
	and.b32  	%r2, %r1, 7;
	setp.lt.u32 	%p3, %r17, 7;
	mov.f64 	%fd35, 0d0000000000000000;
	mov.u32 	%r20, 0;
	@%p3 bra 	LBB9_5;
// %bb.3:                               // %for.body.preheader.new
	sub.s32 	%r3, %r1, %r2;
	add.s64 	%rd10, %rd1, 32;
	mov.f64 	%fd35, 0d0000000000000000;
	mov.u32 	%r20, 0;
LBB9_4:                                 // %for.body
                                        // =>This Inner Loop Header: Depth=1
	ld.global.f64 	%fd15, [%rd10+-32];
	add.rn.f64 	%fd16, %fd35, %fd15;
	ld.global.f64 	%fd17, [%rd10+-24];
	add.rn.f64 	%fd18, %fd16, %fd17;
	ld.global.f64 	%fd19, [%rd10+-16];
	add.rn.f64 	%fd20, %fd18, %fd19;
	ld.global.f64 	%fd21, [%rd10+-8];
	add.rn.f64 	%fd22, %fd20, %fd21;
	ld.global.f64 	%fd23, [%rd10];
	add.rn.f64 	%fd24, %fd22, %fd23;
	ld.global.f64 	%fd25, [%rd10+8];
	add.rn.f64 	%fd26, %fd24, %fd25;
	ld.global.f64 	%fd27, [%rd10+16];
	add.rn.f64 	%fd28, %fd26, %fd27;
	ld.global.f64 	%fd29, [%rd10+24];
	add.rn.f64 	%fd35, %fd28, %fd29;
	add.s32 	%r20, %r20, 8;
	add.s64 	%rd10, %rd10, 64;
	setp.ne.s32 	%p4, %r3, %r20;
	@%p4 bra 	LBB9_4;
LBB9_5:                                 // %for.end.loopexit.unr-lcssa
	setp.eq.s32 	%p5, %r2, 0;
	@%p5 bra 	LBB9_8;
// %bb.6:                               // %for.body.epil.preheader
	mul.wide.u32 	%rd9, %r20, 8;
	add.s64 	%rd11, %rd1, %rd9;
	neg.s32 	%r21, %r2;
LBB9_7:                                 // %for.body.epil
                                        // =>This Inner Loop Header: Depth=1
	.pragma "nounroll";
	ld.global.f64 	%fd30, [%rd11];
	add.rn.f64 	%fd35, %fd35, %fd30;
	add.s64 	%rd11, %rd11, 8;
	add.s32 	%r21, %r21, 1;
	setp.ne.s32 	%p6, %r21, 0;
	@%p6 bra 	LBB9_7;
LBB9_8:                                 // %for.end
	st.global.f64 	[%rd1], %fd35;
LBB9_9:                                 // %if.end
	ret;
                                        // -- End function
}
	// .globl	_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S_ // -- Begin function _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S_
.visible .entry _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S_(
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_0,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_1,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_2,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_3,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_4,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_5,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_6,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_7,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_8,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_9,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_10,
	.param .u32 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_11,
	.param .u32 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_12,
	.param .u32 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_13,
	.param .u32 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_14,
	.param .u32 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_15,
	.param .u32 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_16,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_17,
	.param .u64 _Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_18
)                                       // @_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S_
{
	.local .align 4 .b8 	__local_depot10[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<42>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<193>;
	.reg .f64 	%fd<244>;
	.reg .b64 	%rd<103>;
	// demoted variable
	.shared .align 8 .b8 _ZZ17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S_E6buffer[4096];
// %bb.0:                               // %entry
	mov.u64 	%SPL, __local_depot10;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r49, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_11];
	ld.param.u64 	%rd31, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_0];
	ld.param.u64 	%rd33, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_1];
	ld.param.u64 	%rd36, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_10];
	cvta.to.global.u64 	%rd3, %rd36;
	cvta.to.global.u64 	%rd10, %rd33;
	cvta.to.global.u64 	%rd11, %rd31;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	setp.ge.s32 	%p3, %r4, %r49;
	mov.pred 	%p41, 0;
	@%p3 bra 	LBB10_20;
// %bb.1:                               // %if.then
	ld.param.u64 	%rd34, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_17];
	cvta.to.global.u64 	%rd2, %rd34;
	ld.param.u64 	%rd35, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_2];
	ld.param.u64 	%rd37, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_3];
	cvta.to.global.u64 	%rd8, %rd37;
	cvta.to.global.u64 	%rd9, %rd35;
	add.u64 	%rd42, %SP, 0;
	add.u64 	%rd12, %SPL, 0;
	mul.wide.s32 	%rd44, %r4, 8;
	add.s64 	%rd45, %rd9, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	add.s64 	%rd14, %rd11, %rd44;
	st.global.u64 	[%rd14], %rd46;
	add.s64 	%rd47, %rd8, %rd44;
	ld.global.u64 	%rd48, [%rd47];
	add.s64 	%rd15, %rd10, %rd44;
	st.global.u64 	[%rd15], %rd48;
	cvt.rn.f64.s32 	%fd43, %r49;
	rcp.rn.f64 	%fd44, %fd43;
	add.s64 	%rd49, %rd3, %rd44;
	st.global.f64 	[%rd49], %fd44;
	ld.global.f64 	%fd45, [%rd14];
	mul.wide.s32 	%rd50, %r4, 4;
	add.s64 	%rd16, %rd2, %rd50;
	ld.global.u32 	%r55, [%rd16];
	mad.lo.s32 	%r56, %r55, 1103515245, 12345;
	mul.wide.s32 	%rd51, %r56, 1073741825;
	shr.u64 	%rd52, %rd51, 63;
	cvt.u32.u64 	%r57, %rd52;
	shr.s64 	%rd53, %rd51, 61;
	cvt.u32.u64 	%r58, %rd53;
	add.s32 	%r59, %r58, %r57;
	mul.lo.s32 	%r60, %r59, 2147483647;
	sub.s32 	%r61, %r56, %r60;
	cvt.rn.f64.s32 	%fd46, %r61;
	div.rn.f64 	%fd47, %fd46, 0d41DFFFFFFFC00000;
	abs.f64 	%fd232, %fd47;
	mad.lo.s32 	%r62, %r61, 1103515245, 12345;
	mul.wide.s32 	%rd54, %r62, 1073741825;
	shr.u64 	%rd55, %rd54, 63;
	cvt.u32.u64 	%r63, %rd55;
	shr.s64 	%rd56, %rd54, 61;
	cvt.u32.u64 	%r64, %rd56;
	add.s32 	%r65, %r64, %r63;
	mul.lo.s32 	%r66, %r65, 2147483647;
	sub.s32 	%r67, %r62, %r66;
	st.global.u32 	[%rd16], %r67;
	cvt.rn.f64.s32 	%fd48, %r67;
	div.rn.f64 	%fd49, %fd48, 0d41DFFFFFFFC00000;
	abs.f64 	%fd50, %fd49;
	mul.rn.f64 	%fd51, %fd50, 0d401921FB54442D18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r68, %temp}, %fd51;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd51;
	}
	and.b32  	%r70, %r69, 2147483647;
	setp.eq.s32 	%p4, %r70, 2146435072;
	setp.eq.s32 	%p5, %r68, 0;
	mul.rn.f64 	%fd52, %fd51, 0d0000000000000000;
	selp.f64 	%fd53, %fd52, %fd51, %p4;
	selp.f64 	%fd3, %fd53, %fd51, %p5;
	mul.rn.f64 	%fd54, %fd3, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r178, %fd54;
	st.local.u32 	[%rd12], %r178;
	cvt.rn.f64.s32 	%fd55, %r178;
	fma.rn.f64 	%fd56, %fd55, 0dBFF921FB54442D18, %fd3;
	fma.rn.f64 	%fd57, %fd55, 0dBC91A62633145C00, %fd56;
	fma.rn.f64 	%fd231, %fd55, 0dB97B839A252049C0, %fd57;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r71}, %fd3;
	}
	and.b32  	%r72, %r71, 2145386496;
	setp.lt.u32 	%p6, %r72, 1105199104;
	@%p6 bra 	LBB10_3;
// %bb.2:
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd3;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd42;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd231, [retval0+0];
	} // callseq 1
	ld.local.u32 	%r178, [%rd12];
LBB10_3:                                // %_ZL3cosd.exit.i220
	add.u64 	%rd43, %SP, 4;
	add.s32 	%r74, %r178, 1;
	and.b32  	%r75, %r74, 1;
	shl.b32 	%r76, %r75, 3;
	mul.wide.u32 	%rd58, %r76, 8;
	mov.u64 	%rd59, __cudart_sin_cos_coeffs;
	add.s64 	%rd60, %rd59, %rd58;
	mul.rn.f64 	%fd58, %fd231, %fd231;
	setp.eq.b32 	%p7, %r75, 1;
	selp.f64 	%fd59, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p7;
	ld.const.f64 	%fd60, [%rd60+8];
	fma.rn.f64 	%fd61, %fd59, %fd58, %fd60;
	ld.const.f64 	%fd62, [%rd60+16];
	fma.rn.f64 	%fd63, %fd61, %fd58, %fd62;
	ld.const.f64 	%fd64, [%rd60+24];
	fma.rn.f64 	%fd65, %fd63, %fd58, %fd64;
	ld.const.f64 	%fd66, [%rd60+32];
	fma.rn.f64 	%fd67, %fd65, %fd58, %fd66;
	ld.const.f64 	%fd68, [%rd60+40];
	fma.rn.f64 	%fd69, %fd67, %fd58, %fd68;
	ld.const.f64 	%fd70, [%rd60+48];
	fma.rn.f64 	%fd71, %fd69, %fd58, %fd70;
	fma.rn.f64 	%fd72, %fd71, %fd231, %fd231;
	fma.rn.f64 	%fd73, %fd71, %fd58, 0d3FF0000000000000;
	selp.f64 	%fd74, %fd73, %fd72, %p7;
	and.b32  	%r77, %r74, 2;
	setp.eq.s32 	%p8, %r77, 0;
	mov.f64 	%fd75, 0d0000000000000000;
	sub.rn.f64 	%fd76, %fd75, %fd74;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r179}, %fd232;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r180, %temp}, %fd232;
	}
	setp.gt.s32 	%p9, %r179, 1048575;
	mov.u32 	%r181, -1023;
	@%p9 bra 	LBB10_5;
// %bb.4:
	mul.rn.f64 	%fd232, %fd232, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r179}, %fd232;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r180, %temp}, %fd232;
	}
	mov.u32 	%r181, -1077;
LBB10_5:
	cvta.to.local.u64 	%rd13, %rd43;
	add.rn.f64 	%fd1, %fd45, 0d3FF0000000000000;
	selp.f64 	%fd7, %fd74, %fd76, %p8;
	add.s32 	%r79, %r179, -1;
	setp.gt.u32 	%p10, %r79, 2146435070;
	@%p10 bra 	LBB10_9;
// %bb.6:
	shr.u32 	%r81, %r179, 20;
	add.s32 	%r182, %r181, %r81;
	and.b32  	%r82, %r179, -2146435073;
	or.b32  	%r83, %r82, 1072693248;
	mov.b64 	%fd233, {%r180, %r83};
	setp.lt.s32 	%p12, %r83, 1073127583;
	@%p12 bra 	LBB10_8;
// %bb.7:
	add.s32 	%r182, %r182, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r84, %temp}, %fd233;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd233;
	}
	add.s32 	%r86, %r85, -1048576;
	mov.b64 	%fd233, {%r84, %r86};
LBB10_8:
	add.rn.f64 	%fd78, %fd233, 0dBFF0000000000000;
	add.rn.f64 	%fd79, %fd233, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd80, %fd79;
	neg.f64 	%fd81, %fd79;
	fma.rn.f64 	%fd82, %fd81, %fd80, 0d3FF0000000000000;
	fma.rn.f64 	%fd83, %fd82, %fd82, %fd82;
	fma.rn.f64 	%fd84, %fd83, %fd80, %fd80;
	mul.rn.f64 	%fd85, %fd78, %fd84;
	add.rn.f64 	%fd86, %fd85, %fd85;
	mul.rn.f64 	%fd87, %fd86, %fd86;
	fma.rn.f64 	%fd88, %fd87, 0d3EB1380B3AE80F1E, 0d3ED0EE258B7A8B04;
	fma.rn.f64 	%fd89, %fd88, %fd87, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd90, %fd89, %fd87, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd91, %fd90, %fd87, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd92, %fd91, %fd87, 0d3F624924923BE72D;
	fma.rn.f64 	%fd93, %fd92, %fd87, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd94, %fd93, %fd87, 0d3FB5555555555554;
	sub.rn.f64 	%fd95, %fd78, %fd86;
	add.rn.f64 	%fd96, %fd95, %fd95;
	neg.f64 	%fd97, %fd86;
	fma.rn.f64 	%fd98, %fd97, %fd78, %fd96;
	mul.rn.f64 	%fd99, %fd84, %fd98;
	mul.rn.f64 	%fd100, %fd87, %fd94;
	fma.rn.f64 	%fd101, %fd100, %fd86, %fd99;
	xor.b32  	%r87, %r182, -2147483648;
	mov.u32 	%r88, 1127219200;
	mov.b64 	%fd102, {%r87, %r88};
	mov.u32 	%r89, -2147483648;
	mov.b64 	%fd103, {%r89, %r88};
	sub.rn.f64 	%fd104, %fd102, %fd103;
	fma.rn.f64 	%fd105, %fd104, 0d3FE62E42FEFA39EF, %fd86;
	fma.rn.f64 	%fd106, %fd104, 0dBFE62E42FEFA39EF, %fd105;
	sub.rn.f64 	%fd107, %fd106, %fd86;
	sub.rn.f64 	%fd108, %fd101, %fd107;
	fma.rn.f64 	%fd109, %fd104, 0d3C7ABC9E3B39803F, %fd108;
	add.rn.f64 	%fd234, %fd105, %fd109;
	bra.uni 	LBB10_10;
LBB10_9:
	fma.rn.f64 	%fd77, %fd232, 0d7FF0000000000000, 0d7FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd232;
	}
	mov.b32 	%f2, %r80;
	setp.eq.f32 	%p11, %f2, 0f00000000;
	selp.f64 	%fd234, 0dFFF0000000000000, %fd77, %p11;
LBB10_10:                               // %_Z7d_randnPii.exit232
	mul.rn.f64 	%fd110, %fd234, 0dC000000000000000;
	sqrt.rn.f64 	%fd111, %fd110;
	mul.rn.f64 	%fd112, %fd7, %fd111;
	fma.rn.f64 	%fd113, %fd112, 0d4014000000000000, %fd1;
	st.global.f64 	[%rd14], %fd113;
	ld.global.f64 	%fd114, [%rd15];
	ld.global.u32 	%r90, [%rd16];
	mad.lo.s32 	%r91, %r90, 1103515245, 12345;
	mul.wide.s32 	%rd61, %r91, 1073741825;
	shr.u64 	%rd62, %rd61, 63;
	cvt.u32.u64 	%r92, %rd62;
	shr.s64 	%rd63, %rd61, 61;
	cvt.u32.u64 	%r93, %rd63;
	add.s32 	%r94, %r93, %r92;
	mul.lo.s32 	%r95, %r94, 2147483647;
	sub.s32 	%r96, %r91, %r95;
	cvt.rn.f64.s32 	%fd115, %r96;
	div.rn.f64 	%fd116, %fd115, 0d41DFFFFFFFC00000;
	abs.f64 	%fd236, %fd116;
	mad.lo.s32 	%r97, %r96, 1103515245, 12345;
	mul.wide.s32 	%rd64, %r97, 1073741825;
	shr.u64 	%rd65, %rd64, 63;
	cvt.u32.u64 	%r98, %rd65;
	shr.s64 	%rd66, %rd64, 61;
	cvt.u32.u64 	%r99, %rd66;
	add.s32 	%r100, %r99, %r98;
	mul.lo.s32 	%r101, %r100, 2147483647;
	sub.s32 	%r102, %r97, %r101;
	st.global.u32 	[%rd16], %r102;
	cvt.rn.f64.s32 	%fd117, %r102;
	div.rn.f64 	%fd118, %fd117, 0d41DFFFFFFFC00000;
	abs.f64 	%fd119, %fd118;
	mul.rn.f64 	%fd120, %fd119, 0d401921FB54442D18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r103, %temp}, %fd120;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r104}, %fd120;
	}
	and.b32  	%r105, %r104, 2147483647;
	setp.eq.s32 	%p13, %r105, 2146435072;
	setp.eq.s32 	%p14, %r103, 0;
	mul.rn.f64 	%fd121, %fd120, 0d0000000000000000;
	selp.f64 	%fd122, %fd121, %fd120, %p13;
	selp.f64 	%fd18, %fd122, %fd120, %p14;
	mul.rn.f64 	%fd123, %fd18, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r183, %fd123;
	st.local.u32 	[%rd13], %r183;
	cvt.rn.f64.s32 	%fd124, %r183;
	fma.rn.f64 	%fd125, %fd124, 0dBFF921FB54442D18, %fd18;
	fma.rn.f64 	%fd126, %fd124, 0dBC91A62633145C00, %fd125;
	fma.rn.f64 	%fd235, %fd124, 0dB97B839A252049C0, %fd126;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r106}, %fd18;
	}
	and.b32  	%r107, %r106, 2145386496;
	setp.lt.u32 	%p15, %r107, 1105199104;
	@%p15 bra 	LBB10_12;
// %bb.11:
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd43;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd235, [retval0+0];
	} // callseq 2
	ld.local.u32 	%r183, [%rd13];
LBB10_12:                               // %_ZL3cosd.exit.i
	add.s32 	%r109, %r183, 1;
	and.b32  	%r110, %r109, 1;
	shl.b32 	%r111, %r110, 3;
	mul.wide.u32 	%rd68, %r111, 8;
	add.s64 	%rd70, %rd59, %rd68;
	mul.rn.f64 	%fd127, %fd235, %fd235;
	setp.eq.b32 	%p16, %r110, 1;
	selp.f64 	%fd128, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p16;
	ld.const.f64 	%fd129, [%rd70+8];
	fma.rn.f64 	%fd130, %fd128, %fd127, %fd129;
	ld.const.f64 	%fd131, [%rd70+16];
	fma.rn.f64 	%fd132, %fd130, %fd127, %fd131;
	ld.const.f64 	%fd133, [%rd70+24];
	fma.rn.f64 	%fd134, %fd132, %fd127, %fd133;
	ld.const.f64 	%fd135, [%rd70+32];
	fma.rn.f64 	%fd136, %fd134, %fd127, %fd135;
	ld.const.f64 	%fd137, [%rd70+40];
	fma.rn.f64 	%fd138, %fd136, %fd127, %fd137;
	ld.const.f64 	%fd139, [%rd70+48];
	fma.rn.f64 	%fd140, %fd138, %fd127, %fd139;
	fma.rn.f64 	%fd141, %fd140, %fd235, %fd235;
	fma.rn.f64 	%fd142, %fd140, %fd127, 0d3FF0000000000000;
	selp.f64 	%fd143, %fd142, %fd141, %p16;
	and.b32  	%r112, %r109, 2;
	setp.eq.s32 	%p17, %r112, 0;
	sub.rn.f64 	%fd145, %fd75, %fd143;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r184}, %fd236;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r185, %temp}, %fd236;
	}
	setp.gt.s32 	%p18, %r184, 1048575;
	mov.u32 	%r186, -1023;
	@%p18 bra 	LBB10_14;
// %bb.13:
	mul.rn.f64 	%fd236, %fd236, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r184}, %fd236;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r185, %temp}, %fd236;
	}
	mov.u32 	%r186, -1077;
LBB10_14:
	add.rn.f64 	%fd16, %fd114, 0dC000000000000000;
	selp.f64 	%fd22, %fd143, %fd145, %p17;
	add.s32 	%r114, %r184, -1;
	setp.gt.u32 	%p19, %r114, 2146435070;
	@%p19 bra 	LBB10_18;
// %bb.15:
	shr.u32 	%r116, %r184, 20;
	add.s32 	%r187, %r186, %r116;
	and.b32  	%r117, %r184, -2146435073;
	or.b32  	%r118, %r117, 1072693248;
	mov.b64 	%fd237, {%r185, %r118};
	setp.lt.s32 	%p21, %r118, 1073127583;
	@%p21 bra 	LBB10_17;
// %bb.16:
	add.s32 	%r187, %r187, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r119, %temp}, %fd237;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r120}, %fd237;
	}
	add.s32 	%r121, %r120, -1048576;
	mov.b64 	%fd237, {%r119, %r121};
LBB10_17:
	add.rn.f64 	%fd147, %fd237, 0dBFF0000000000000;
	add.rn.f64 	%fd148, %fd237, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd149, %fd148;
	neg.f64 	%fd150, %fd148;
	fma.rn.f64 	%fd151, %fd150, %fd149, 0d3FF0000000000000;
	fma.rn.f64 	%fd152, %fd151, %fd151, %fd151;
	fma.rn.f64 	%fd153, %fd152, %fd149, %fd149;
	mul.rn.f64 	%fd154, %fd147, %fd153;
	add.rn.f64 	%fd155, %fd154, %fd154;
	mul.rn.f64 	%fd156, %fd155, %fd155;
	fma.rn.f64 	%fd157, %fd156, 0d3EB1380B3AE80F1E, 0d3ED0EE258B7A8B04;
	fma.rn.f64 	%fd158, %fd157, %fd156, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd159, %fd158, %fd156, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd160, %fd159, %fd156, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd161, %fd160, %fd156, 0d3F624924923BE72D;
	fma.rn.f64 	%fd162, %fd161, %fd156, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd163, %fd162, %fd156, 0d3FB5555555555554;
	sub.rn.f64 	%fd164, %fd147, %fd155;
	add.rn.f64 	%fd165, %fd164, %fd164;
	neg.f64 	%fd166, %fd155;
	fma.rn.f64 	%fd167, %fd166, %fd147, %fd165;
	mul.rn.f64 	%fd168, %fd153, %fd167;
	mul.rn.f64 	%fd169, %fd156, %fd163;
	fma.rn.f64 	%fd170, %fd169, %fd155, %fd168;
	xor.b32  	%r122, %r187, -2147483648;
	mov.u32 	%r123, 1127219200;
	mov.b64 	%fd171, {%r122, %r123};
	mov.u32 	%r124, -2147483648;
	mov.b64 	%fd172, {%r124, %r123};
	sub.rn.f64 	%fd173, %fd171, %fd172;
	fma.rn.f64 	%fd174, %fd173, 0d3FE62E42FEFA39EF, %fd155;
	fma.rn.f64 	%fd175, %fd173, 0dBFE62E42FEFA39EF, %fd174;
	sub.rn.f64 	%fd176, %fd175, %fd155;
	sub.rn.f64 	%fd177, %fd170, %fd176;
	fma.rn.f64 	%fd178, %fd173, 0d3C7ABC9E3B39803F, %fd177;
	add.rn.f64 	%fd238, %fd174, %fd178;
	bra.uni 	LBB10_19;
LBB10_18:
	fma.rn.f64 	%fd146, %fd236, 0d7FF0000000000000, 0d7FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r115}, %fd236;
	}
	mov.b32 	%f3, %r115;
	setp.eq.f32 	%p20, %f3, 0f00000000;
	selp.f64 	%fd238, 0dFFF0000000000000, %fd146, %p20;
LBB10_19:                               // %_Z7d_randnPii.exit
	mul.rn.f64 	%fd179, %fd238, 0dC000000000000000;
	sqrt.rn.f64 	%fd180, %fd179;
	mul.rn.f64 	%fd181, %fd22, %fd180;
	fma.rn.f64 	%fd182, %fd181, 0d4000000000000000, %fd16;
	st.global.f64 	[%rd15], %fd182;
	mov.pred 	%p41, -1;
LBB10_20:                               // %if.end
	bar.sync 	0;
	@!%p41 bra 	LBB10_36;
	bra.uni 	LBB10_21;
LBB10_21:                               // %for.cond.preheader
	ld.param.u64 	%rd39, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_7];
	ld.param.u32 	%r50, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_12];
	cvta.to.global.u64 	%rd5, %rd39;
	setp.lt.s32 	%p23, %r50, 1;
	mov.f64 	%fd240, 0d0000000000000000;
	@%p23 bra 	LBB10_32;
// %bb.22:                              // %for.body.lr.ph
	ld.param.u64 	%rd40, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_5];
	ld.param.u64 	%rd41, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_6];
	ld.param.u32 	%r54, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_16];
	ld.param.u32 	%r53, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_15];
	ld.param.u32 	%r52, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_14];
	ld.param.u32 	%r51, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_13];
	cvta.to.global.u64 	%rd6, %rd41;
	cvta.to.global.u64 	%rd7, %rd40;
	mul.wide.s32 	%rd71, %r4, 8;
	add.s64 	%rd72, %rd11, %rd71;
	ld.global.f64 	%fd184, [%rd72];
	cvt.rzi.s32.f64 	%r31, %fd184;
	add.s64 	%rd73, %rd10, %rd71;
	ld.global.f64 	%fd185, [%rd73];
	cvt.rzi.s32.f64 	%r32, %fd185;
	mul.lo.s32 	%r33, %r4, %r50;
	and.b32  	%r34, %r50, 1;
	setp.eq.s32 	%p24, %r50, 1;
	mov.u32 	%r189, 0;
	@%p24 bra 	LBB10_25;
// %bb.23:                              // %for.body.lr.ph.new
	sub.s32 	%r35, %r50, %r34;
	add.s64 	%rd101, %rd6, 8;
	mul.wide.s32 	%rd74, %r33, 4;
	add.s64 	%rd75, %rd7, %rd74;
	add.s64 	%rd100, %rd75, 4;
	mov.u32 	%r189, 0;
LBB10_24:                               // %for.body
                                        // =>This Inner Loop Header: Depth=1
	ld.global.u32 	%r127, [%rd101+-4];
	add.s32 	%r128, %r127, %r31;
	ld.global.u32 	%r129, [%rd101+-8];
	add.s32 	%r130, %r129, %r32;
	mad.lo.s32 	%r131, %r128, %r53, %r130;
	mad.lo.s32 	%r132, %r131, %r54, %r52;
	abs.s32 	%r133, %r132;
	setp.lt.s32 	%p25, %r133, %r51;
	selp.b32 	%r134, %r133, 0, %p25;
	st.global.u32 	[%rd100+-4], %r134;
	ld.global.u32 	%r135, [%rd101+4];
	add.s32 	%r136, %r135, %r31;
	ld.global.u32 	%r137, [%rd101];
	add.s32 	%r138, %r137, %r32;
	mad.lo.s32 	%r139, %r136, %r53, %r138;
	mad.lo.s32 	%r140, %r139, %r54, %r52;
	abs.s32 	%r141, %r140;
	setp.lt.s32 	%p26, %r141, %r51;
	selp.b32 	%r142, %r141, 0, %p26;
	st.global.u32 	[%rd100], %r142;
	add.s32 	%r189, %r189, 2;
	add.s64 	%rd101, %rd101, 16;
	add.s64 	%rd100, %rd100, 8;
	setp.ne.s32 	%p27, %r35, %r189;
	@%p27 bra 	LBB10_24;
LBB10_25:                               // %for.end.unr-lcssa
	ld.param.u64 	%rd38, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_8];
	setp.eq.s32 	%p28, %r34, 0;
	@%p28 bra 	LBB10_27;
// %bb.26:                              // %for.body.epil
	shl.b32 	%r143, %r189, 1;
	mul.wide.u32 	%rd76, %r143, 4;
	add.s64 	%rd77, %rd6, %rd76;
	ld.global.u32 	%r144, [%rd77+4];
	add.s32 	%r145, %r144, %r31;
	ld.global.u32 	%r146, [%rd77];
	add.s32 	%r147, %r146, %r32;
	mad.lo.s32 	%r148, %r145, %r53, %r147;
	mad.lo.s32 	%r149, %r148, %r54, %r52;
	abs.s32 	%r150, %r149;
	add.s32 	%r151, %r189, %r33;
	mul.wide.s32 	%rd78, %r151, 4;
	add.s64 	%rd79, %rd7, %rd78;
	setp.lt.s32 	%p29, %r150, %r51;
	selp.b32 	%r152, %r150, 0, %p29;
	st.global.u32 	[%rd79], %r152;
LBB10_27:                               // %for.end
	cvta.to.global.u64 	%rd4, %rd38;
	mov.f64 	%fd240, 0d0000000000000000;
	mov.u32 	%r191, 0;
	@%p24 bra 	LBB10_30;
// %bb.28:                              // %for.body.lr.ph.i.new
	sub.s32 	%r39, %r50, %r34;
	mul.wide.s32 	%rd80, %r33, 4;
	add.s64 	%rd81, %rd7, %rd80;
	add.s64 	%rd102, %rd81, 4;
	mov.f64 	%fd240, 0d0000000000000000;
	mov.u32 	%r191, 0;
LBB10_29:                               // %for.body.i
                                        // =>This Inner Loop Header: Depth=1
	ld.global.s32 	%rd82, [%rd102+-4];
	add.s64 	%rd83, %rd4, %rd82;
	ld.global.u8 	%r155, [%rd83];
	add.s32 	%r156, %r155, -100;
	cvt.rn.f64.s32 	%fd189, %r156;
	add.s32 	%r157, %r155, -228;
	cvt.rn.f64.s32 	%fd190, %r157;
	mul.rn.f64 	%fd191, %fd190, %fd190;
	neg.f64 	%fd192, %fd191;
	fma.rn.f64 	%fd193, %fd189, %fd189, %fd192;
	div.rn.f64 	%fd194, %fd193, 0d4049000000000000;
	add.rn.f64 	%fd195, %fd240, %fd194;
	ld.global.s32 	%rd84, [%rd102];
	add.s64 	%rd85, %rd4, %rd84;
	ld.global.u8 	%r158, [%rd85];
	add.s32 	%r159, %r158, -100;
	cvt.rn.f64.s32 	%fd196, %r159;
	add.s32 	%r160, %r158, -228;
	cvt.rn.f64.s32 	%fd197, %r160;
	mul.rn.f64 	%fd198, %fd197, %fd197;
	neg.f64 	%fd199, %fd198;
	fma.rn.f64 	%fd200, %fd196, %fd196, %fd199;
	div.rn.f64 	%fd201, %fd200, 0d4049000000000000;
	add.rn.f64 	%fd240, %fd195, %fd201;
	add.s32 	%r191, %r191, 2;
	add.s64 	%rd102, %rd102, 8;
	setp.ne.s32 	%p31, %r39, %r191;
	@%p31 bra 	LBB10_29;
LBB10_30:                               // %_Z17calcLikelihoodSumPhPiii.exit.loopexit.unr-lcssa
	@%p28 bra 	LBB10_32;
// %bb.31:                              // %for.body.i.epil
	add.s32 	%r161, %r191, %r33;
	mul.wide.s32 	%rd86, %r161, 4;
	add.s64 	%rd87, %rd7, %rd86;
	ld.global.s32 	%rd88, [%rd87];
	add.s64 	%rd89, %rd4, %rd88;
	ld.global.u8 	%r162, [%rd89];
	add.s32 	%r163, %r162, -100;
	cvt.rn.f64.s32 	%fd202, %r163;
	add.s32 	%r164, %r162, -228;
	cvt.rn.f64.s32 	%fd203, %r164;
	mul.rn.f64 	%fd204, %fd203, %fd203;
	neg.f64 	%fd205, %fd204;
	fma.rn.f64 	%fd206, %fd202, %fd202, %fd205;
	div.rn.f64 	%fd207, %fd206, 0d4049000000000000;
	add.rn.f64 	%fd240, %fd240, %fd207;
LBB10_32:                               // %_Z17calcLikelihoodSumPhPiii.exit
	mul.wide.s32 	%rd90, %r4, 8;
	add.s64 	%rd91, %rd5, %rd90;
	cvt.rn.f64.s32 	%fd208, %r50;
	div.rn.f64 	%fd37, %fd240, %fd208;
	st.global.f64 	[%rd91], %fd37;
	add.s64 	%rd26, %rd3, %rd90;
	ld.global.f64 	%fd38, [%rd26];
	fma.rn.f64 	%fd209, %fd37, 0d3FF71547652B82FE, 0d4338000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd209;
	}
	add.rn.f64 	%fd210, %fd209, 0dC338000000000000;
	fma.rn.f64 	%fd211, %fd210, 0dBFE62E42FEFA39EF, %fd37;
	fma.rn.f64 	%fd212, %fd210, 0dBC7ABC9E3B39803F, %fd211;
	fma.rn.f64 	%fd213, %fd212, 0d3E5ADE1569CE2BDF, 0d3E928AF3FCA213EA;
	fma.rn.f64 	%fd214, %fd213, %fd212, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd215, %fd214, %fd212, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd216, %fd215, %fd212, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd217, %fd216, %fd212, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd218, %fd217, %fd212, 0d3F81111111122322;
	fma.rn.f64 	%fd219, %fd218, %fd212, 0d3FA55555555502A1;
	fma.rn.f64 	%fd220, %fd219, %fd212, 0d3FC5555555555511;
	fma.rn.f64 	%fd221, %fd220, %fd212, 0d3FE000000000000B;
	fma.rn.f64 	%fd222, %fd221, %fd212, 0d3FF0000000000000;
	fma.rn.f64 	%fd223, %fd222, %fd212, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd223;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd223;
	}
	shl.b32 	%r165, %r43, 20;
	add.s32 	%r166, %r45, %r165;
	mov.b64 	%fd243, {%r44, %r166};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r167}, %fd37;
	}
	and.b32  	%r168, %r167, 2147483647;
	mov.b32 	%f1, %r168;
	setp.lt.f32 	%p33, %f1, 0f4086232B;
	@%p33 bra 	LBB10_35;
// %bb.33:                              // %__internal_fast_icmp_abs_lt.exit.i.i
	setp.lt.f64 	%p34, %fd37, 0d0000000000000000;
	add.rn.f64 	%fd224, %fd37, 0d7FF0000000000000;
	selp.f64 	%fd243, 0d0000000000000000, %fd224, %p34;
	setp.geu.f32 	%p35, %f1, 0f40874800;
	@%p35 bra 	LBB10_35;
// %bb.34:
	shr.u32 	%r169, %r43, 31;
	add.s32 	%r170, %r43, %r169;
	shr.s32 	%r171, %r170, 1;
	shl.b32 	%r172, %r171, 20;
	add.s32 	%r173, %r45, %r172;
	mov.b64 	%fd225, {%r44, %r173};
	sub.s32 	%r174, %r43, %r171;
	shl.b32 	%r175, %r174, 20;
	add.s32 	%r176, %r175, 1072693248;
	mov.u32 	%r177, 0;
	mov.b64 	%fd226, {%r177, %r176};
	mul.rn.f64 	%fd243, %fd225, %fd226;
LBB10_35:                               // %_ZL3expd.exit
	mul.rn.f64 	%fd227, %fd38, %fd243;
	st.global.f64 	[%rd26], %fd227;
LBB10_36:                               // %if.end86
	mul.wide.u32 	%rd92, %r3, 8;
	mov.u64 	%rd93, _ZZ17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S_E6buffer;
	add.s64 	%rd27, %rd93, %rd92;
	mov.u64 	%rd94, 0;
	st.shared.u64 	[%rd27], %rd94;
	bar.sync 	0;
	@%p3 bra 	LBB10_38;
// %bb.37:                              // %if.then91
	mul.wide.s32 	%rd95, %r4, 8;
	add.s64 	%rd28, %rd3, %rd95;
	ld.global.u64 	%rd96, [%rd28];
	st.shared.u64 	[%rd27], %rd96;
LBB10_38:                               // %if.end97
	bar.sync 	0;
	shr.u32 	%r192, %r2, 1;
	setp.eq.s32 	%p37, %r192, 0;
	@%p37 bra 	LBB10_39;
LBB10_42:                               // %for.body102
                                        // =>This Inner Loop Header: Depth=1
	setp.ge.u32 	%p38, %r3, %r192;
	@%p38 bra 	LBB10_44;
// %bb.43:                              // %if.then105
                                        //   in Loop: Header=BB10_42 Depth=1
	mul.wide.s32 	%rd97, %r192, 8;
	add.s64 	%rd30, %rd27, %rd97;
	ld.shared.f64 	%fd228, [%rd30];
	ld.shared.f64 	%fd229, [%rd27];
	add.rn.f64 	%fd230, %fd228, %fd229;
	st.shared.f64 	[%rd27], %fd230;
LBB10_44:                               // %if.end114
                                        //   in Loop: Header=BB10_42 Depth=1
	bar.sync 	0;
	shr.u32 	%r192, %r192, 1;
	setp.eq.s32 	%p39, %r192, 0;
	@%p39 bra 	LBB10_39;
	bra.uni 	LBB10_42;
LBB10_39:                               // %for.cond.cleanup
	setp.eq.s32 	%p40, %r3, 0;
	@%p40 bra 	LBB10_40;
	bra.uni 	LBB10_41;
LBB10_40:                               // %if.then119
	ld.param.u64 	%rd32, [_Z17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S__param_18];
	cvta.to.global.u64 	%rd1, %rd32;
	mul.wide.u32 	%rd98, %r1, 8;
	add.s64 	%rd29, %rd1, %rd98;
	ld.shared.u64 	%rd99, [_ZZ17likelihood_kernelPdS_S_S_S_PiS0_S_PhS_S_iiiiiiS0_S_E6buffer];
	st.global.u64 	[%rd29], %rd99;
LBB10_41:                               // %if.end123
	bar.sync 	0;
	ret;
                                        // -- End function
}
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)                                       // -- Begin function __internal_trig_reduction_slowpathd
                                        // @__internal_trig_reduction_slowpathd
{
	.local .align 8 .b8 	__local_depot11[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<107>;

// %bb.0:
	mov.u64 	%SPL, __local_depot11;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd4;
	}
	bfe.u32 	%r2, %r12, 20, 11;
	setp.eq.s32 	%p1, %r2, 2047;
	@%p1 bra 	LBB11_11;
// %bb.1:
	add.u64 	%rd1, %SPL, 0;
	add.s32 	%r3, %r2, -1024;
	shr.u32 	%r4, %r3, 6;
	mov.u32 	%r13, 16;
	sub.s32 	%r14, %r13, %r4;
	mov.u32 	%r15, 15;
	sub.s32 	%r33, %r15, %r4;
	mov.u32 	%r16, 19;
	sub.s32 	%r17, %r16, %r4;
	setp.gt.s32 	%p2, %r14, 14;
	selp.b32 	%r6, 18, %r17, %p2;
	setp.gt.s32 	%p3, %r14, %r6;
	mov.u64 	%rd100, 0;
	@%p3 bra 	LBB11_4;
// %bb.2:                               // %.lr.ph.preheader
	mov.b64 	%rd36, %fd4;
	shl.b64 	%rd37, %rd36, 11;
	or.b64  	%rd50, %rd37, -9223372036854775808;
	cvt.u64.u32 	%rd39, %r3;
	shr.u64 	%rd40, %rd39, 6;
	sub.s32 	%r19, %r15, %r4;
	cvt.s64.s32 	%rd41, %r19;
	add.s64 	%rd42, %rd40, %rd41;
	shl.b64 	%rd43, %rd42, 3;
	add.s64 	%rd44, %rd1, %rd43;
	add.s64 	%rd98, %rd44, -120;
	mul.wide.s32 	%rd45, %r19, 8;
	mov.u64 	%rd46, __cudart_i2opi_d;
	add.s64 	%rd97, %rd46, %rd45;
	mov.u64 	%rd100, 0;
LBB11_3:                                // %.lr.ph
                                        // =>This Inner Loop Header: Depth=1
	ld.const.u64 	%rd49, [%rd97];
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd49;    
	mov.b64         {blo,bhi}, %rd50;    
	mov.b64         {clo,chi}, %rd100;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd47, {r0,r1};      
	mov.b64         %rd100, {r2,r3};      
	}
	// end inline asm
	st.local.u64 	[%rd98], %rd47;
	add.s32 	%r33, %r33, 1;
	add.s64 	%rd98, %rd98, 8;
	add.s64 	%rd97, %rd97, 8;
	setp.lt.s32 	%p4, %r33, %r6;
	@%p4 bra 	LBB11_3;
LBB11_4:                                // %._crit_edge
	ld.param.u64 	%rd33, [__internal_trig_reduction_slowpathd_param_1];
	and.b32  	%r34, %r12, -2147483648;
	cvt.s64.s32 	%rd52, %r33;
	cvt.u64.u32 	%rd53, %r4;
	add.s64 	%rd54, %rd52, %rd53;
	shl.b64 	%rd55, %rd54, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.local.u64 	[%rd56+-120], %rd100;
	and.b32  	%r20, %r3, 63;
	ld.local.u64 	%rd101, [%rd1+16];
	ld.local.u64 	%rd102, [%rd1+24];
	setp.eq.s32 	%p5, %r20, 0;
	@%p5 bra 	LBB11_6;
// %bb.5:
	mov.u32 	%r21, 64;
	sub.s32 	%r22, %r21, %r20;
	cvt.u64.u32 	%rd14, %r22;
	shl.b64 	%rd15, %rd101, %r20;
	shl.b64 	%rd57, %rd102, %r20;
	shr.u64 	%rd58, %rd101, %r22;
	or.b64  	%rd102, %rd57, %rd58;
	ld.local.u64 	%rd59, [%rd1+8];
	cvt.u32.u64 	%r23, %rd14;
	shr.u64 	%rd60, %rd59, %r23;
	or.b64  	%rd101, %rd60, %rd15;
LBB11_6:
	shr.u64 	%rd61, %rd102, 62;
	cvt.u32.u64 	%r24, %rd61;
	shl.b64 	%rd62, %rd102, 2;
	shr.u64 	%rd63, %rd101, 62;
	or.b64  	%rd104, %rd62, %rd63;
	shl.b64 	%rd103, %rd101, 2;
	shr.u64 	%rd64, %rd102, 61;
	cvt.u32.u64 	%r25, %rd64;
	and.b32  	%r26, %r25, 1;
	add.s32 	%r27, %r26, %r24;
	setp.eq.s32 	%p6, %r34, 0;
	neg.s32 	%r28, %r27;
	selp.b32 	%r29, %r27, %r28, %p6;
	st.u32 	[%rd33], %r29;
	setp.eq.s32 	%p7, %r26, 0;
	@%p7 bra 	LBB11_8;
// %bb.7:
	xor.b32  	%r34, %r34, -2147483648;
	mov.u64 	%rd67, 0;
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd67;
	mov.b64         {a2,a3}, %rd67;
	mov.b64         {b0,b1}, %rd103;
	mov.b64         {b2,b3}, %rd104;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd103, {r0,r1};
	mov.b64         %rd104, {r2,r3};
	}
	// end inline asm
LBB11_8:
	clz.b64 	%r30, %rd104;
	cvt.u64.u32 	%rd105, %r30;
	setp.eq.s64 	%p8, %rd105, 0;
	shl.b64 	%rd75, %rd104, %r30;
	mov.u64 	%rd76, 64;
	sub.s64 	%rd77, %rd76, %rd105;
	cvt.u32.u64 	%r31, %rd77;
	shr.u64 	%rd78, %rd103, %r31;
	or.b64  	%rd79, %rd78, %rd75;
	selp.b64 	%rd73, %rd104, %rd79, %p8;
	mov.u64 	%rd74, -3958705157555305931;
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd73;   
	mov.b64         {blo,bhi}, %rd74;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd82, {r0,r1};     
	mov.b64         %rd106, {r2,r3};     
	}
	// end inline asm
	setp.lt.s64 	%p9, %rd106, 1;
	@%p9 bra 	LBB11_10;
// %bb.9:
	add.s64 	%rd105, %rd105, 1;
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd82;
	mov.b64         {a2,a3}, %rd106;
	mov.b64         {b0,b1}, %rd82;
	mov.b64         {b2,b3}, %rd106;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd80, {r0,r1};
	mov.b64         %rd106, {r2,r3};
	}
	// end inline asm
LBB11_10:
	cvt.u64.u32 	%rd86, %r34;
	shl.b64 	%rd87, %rd86, 32;
	shl.b64 	%rd88, %rd105, 52;
	mov.u64 	%rd89, 4602678819172646912;
	sub.s64 	%rd90, %rd89, %rd88;
	add.s64 	%rd91, %rd106, 1;
	shr.u64 	%rd92, %rd91, 10;
	add.s64 	%rd93, %rd92, 1;
	shr.u64 	%rd94, %rd93, 1;
	add.s64 	%rd95, %rd90, %rd94;
	or.b64  	%rd96, %rd95, %rd87;
	mov.b64 	%fd4, %rd96;
LBB11_11:
	st.param.f64 	[func_retval0+0], %fd4;
	ret;
                                        // -- End function
}

